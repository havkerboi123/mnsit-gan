{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:03<00:00, 2479227.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 124016.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 1058054.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 86919.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = datasets.MNIST(root='./data', \n",
    "                          train=True,\n",
    "                          download=True,\n",
    "                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
      "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
      "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
      "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
      "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
      "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
      "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
      "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
      "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
      "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
      "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
      "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
      "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
      "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "       dtype=torch.uint8)\n",
      "tensor(5)\n",
      "tensor(255, dtype=torch.uint8)\n",
      "tensor(0, dtype=torch.uint8)\n",
      "tensor(35.1084)\n",
      "tensor(79.6997)\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.data.shape)\n",
    "print(train_ds.targets.shape)\n",
    "print(train_ds.classes)\n",
    "print(train_ds.data[0])\n",
    "print(train_ds.targets[0])\n",
    "print(train_ds.data[0].max())\n",
    "print(train_ds.data[0].min())\n",
    "print(train_ds.data[0].float().mean())\n",
    "print(train_ds.data[0].float().std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset=train_ds,\n",
    "                shuffle=True,\n",
    "                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Discriminator will down-sample the input producing a binary output\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=128)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.leaky_relu3 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc4 = nn.Linear(in_features=32, out_features=out_features)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Rehape passed image batch\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        # Feed forward\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.dropout(x)                        \n",
    "        x = self.fc3(x)\n",
    "        x = self.leaky_relu3(x)        \n",
    "        x = self.dropout(x)\n",
    "        logit_out = self.fc4(x)\n",
    "        \n",
    "        return logit_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Generator will up-sample the input producing input of size\n",
    "        # suitable for feeding into discriminator\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=32)\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=64)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=128)\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope=0.2)        \n",
    "        self.fc4 = nn.Linear(in_features=128, out_features=out_features)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feed forward\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        tanh_out = self.tanh(x)\n",
    "        \n",
    "        return tanh_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(predicted_outputs, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Function for calculating loss when samples are drawn from real dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_outputs: Tensor\n",
    "                       predicted logits\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    real_loss: int\n",
    "    \"\"\"\n",
    "    batch_size = predicted_outputs.shape[0]\n",
    "    # Targets are set to 1 here because we expect prediction to be \n",
    "    # 1 (or near 1) since samples are drawn from real dataset\n",
    "    targets = torch.ones(batch_size).to(device)\n",
    "    real_loss = loss_fn(predicted_outputs.squeeze(), targets)\n",
    "    \n",
    "    return real_loss\n",
    "\n",
    "\n",
    "def fake_loss(predicted_outputs, loss_fn, device):\n",
    "    \"\"\"\n",
    "    Function for calculating loss when samples are generated fake samples\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted_outputs: Tensor\n",
    "                       predicted logits\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    fake_loss: int\n",
    "    \"\"\"\n",
    "    batch_size = predicted_outputs.shape[0]\n",
    "    # Targets are set to 0 here because we expect prediction to be \n",
    "    # 0 (or near 0) since samples are generated fake samples\n",
    "    targets = torch.zeros(batch_size).to(device)\n",
    "    fake_loss = loss_fn(predicted_outputs.squeeze(), targets)\n",
    "    \n",
    "    return fake_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_size = 100\n",
    "z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "plt.imshow(z, cmap='gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minst_gan(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device, verbose=False):\n",
    "    print(f'Training on [{device}]...')\n",
    "    \n",
    "    # Generate a batch (say 16) of latent image vector (z) of fixed size \n",
    "    # (say 100 pix) to be as input to the Generator after each epoch of \n",
    "    # training to generate a fake image. We'll visualise these fake images\n",
    "    # to get a sense how generator improves as training progresses\n",
    "    z_size = 100\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(16, z_size))\n",
    "    fixed_z = torch.from_numpy(fixed_z).float().to(device)          \n",
    "    fixed_samples = []\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    \n",
    "    # Move discriminator and generator to available device\n",
    "    d = d.to(device)\n",
    "    g = g.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]:')\n",
    "        # Switch the training mode on\n",
    "        d.train()\n",
    "        g.train()\n",
    "        d_running_batch_loss = 0\n",
    "        g_running_batch_loss = 0\n",
    "        for curr_batch, (real_images, _) in enumerate(dl):\n",
    "            # Move input batch to available device\n",
    "            real_images = real_images.to(device)\n",
    "            \n",
    "            ## ----------------------------------------------------------------\n",
    "            ## Train discriminator using real and then fake MNIST images,  \n",
    "            ## then compute the total-loss and back-propogate the total-loss\n",
    "            ## ----------------------------------------------------------------\n",
    "            \n",
    "            # Reset gradients\n",
    "            d_optim.zero_grad()\n",
    "            \n",
    "            # Real MNIST images\n",
    "            # Convert real_images value range of 0 to 1 to -1 to 1\n",
    "            # this is required because latter discriminator would be required \n",
    "            # to consume generator's 'tanh' output which is of range -1 to 1\n",
    "            real_images = (real_images * 2) - 1  \n",
    "            d_real_logits_out = d(real_images)\n",
    "            d_real_loss = real_loss(d_real_logits_out, loss_fn, device)\n",
    "            #d_real_loss = real_loss(d_real_logits_out, smooth=True)\n",
    "            \n",
    "            # Fake images\n",
    "            with torch.no_grad():\n",
    "                # Generate a batch of random latent vectors \n",
    "                z = np.random.uniform(-1, 1, size=(dl.batch_size, z_size))\n",
    "                z = torch.from_numpy(z).float().to(device)\n",
    "                # Generate batch of fake images\n",
    "                fake_images = g(z) \n",
    "            # feed fake-images to discriminator and compute the \n",
    "            # fake_loss (i.e. target label = 0)\n",
    "            d_fake_logits_out = d(fake_images)\n",
    "            d_fake_loss = fake_loss(d_fake_logits_out, loss_fn, device)\n",
    "            #d_fake_loss = fake_loss(d_fake_logits_out)\n",
    "            # Compute total discriminator loss\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            # Backpropogate through discriminator\n",
    "            d_loss.backward()\n",
    "            d_optim.step()\n",
    "            # Save discriminator batch loss\n",
    "            d_running_batch_loss += d_loss\n",
    "            \n",
    "            ## ----------------------------------------------------------------\n",
    "            ## Train generator, compute the generator loss which is a measure\n",
    "            ## of how successful the generator is in tricking the discriminator \n",
    "            ## and finally back-propogate generator loss\n",
    "            ## ----------------------------------------------------------------\n",
    "\n",
    "            # Reset gradients\n",
    "            g_optim.zero_grad()\n",
    "            \n",
    "            # Generate a batch of random latent vectors\n",
    "            #z = torch.rand(size=(dl.batch_size, z_size)).to(device)\n",
    "            z = np.random.uniform(-1, 1, size=(dl.batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float().to(device)       \n",
    "            # Generate a batch of fake images, feed them to discriminator\n",
    "            # and compute the generator loss as real_loss \n",
    "            # (i.e. target label = 1)\n",
    "            fake_images = g(z) \n",
    "            g_logits_out = d(fake_images)\n",
    "            g_loss = real_loss(g_logits_out, loss_fn, device)\n",
    "            #g_loss = real_loss(g_logits_out)\n",
    "            # Backpropogate thorugh generator\n",
    "            g_loss.backward()\n",
    "            g_optim.step()\n",
    "            # Save discriminator batch loss\n",
    "            g_running_batch_loss += g_loss\n",
    "            \n",
    "            # Display training stats for every 200 batches \n",
    "            if curr_batch % 400 == 0 and verbose:\n",
    "                print(f'\\tBatch [{curr_batch:>4}/{len(dl):>4}] - d_batch_loss: {d_loss.item():.6f}\\tg_batch_loss: {g_loss.item():.6f}')\n",
    "            \n",
    "        # Compute epoch losses as total_batch_loss/number_of_batches\n",
    "        d_epoch_loss = d_running_batch_loss.item()/len(dl)\n",
    "        g_epoch_loss = g_running_batch_loss.item()/len(dl)\n",
    "        d_losses.append(d_epoch_loss)\n",
    "        g_losses.append(g_epoch_loss)\n",
    "        \n",
    "        # Display training stats for every 200 batches \n",
    "        print(f'epoch_d_loss: {d_epoch_loss:.6f} \\tepoch_g_loss: {g_epoch_loss:.6f}')\n",
    "        \n",
    "        # Generate fake images from fixed latent vector using the trained \n",
    "        # generator so far and save images for latter viewing\n",
    "        g.eval()\n",
    "        fixed_samples.append(g(fixed_z).detach().cpu())\n",
    "        \n",
    "    # Finally write generated fake images from fixed latent vector to disk\n",
    "    with open('fixed_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(fixed_samples, f)\n",
    "     \n",
    "    return d_losses, g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (leaky_relu1): LeakyReLU(negative_slope=0.2)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (leaky_relu2): LeakyReLU(negative_slope=0.2)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (leaky_relu3): LeakyReLU(negative_slope=0.2)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (fc1): Linear(in_features=100, out_features=32, bias=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.2)\n",
      "  (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (relu2): LeakyReLU(negative_slope=0.2)\n",
      "  (fc3): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (relu3): LeakyReLU(negative_slope=0.2)\n",
      "  (fc4): Linear(in_features=128, out_features=784, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "Training on [cpu]...\n",
      "Epoch [1/100]:\n",
      "epoch_d_loss: 1.201450 \tepoch_g_loss: 1.544855\n",
      "Epoch [2/100]:\n",
      "epoch_d_loss: 1.325401 \tepoch_g_loss: 0.929858\n",
      "Epoch [3/100]:\n",
      "epoch_d_loss: 1.339285 \tepoch_g_loss: 0.919249\n",
      "Epoch [4/100]:\n",
      "epoch_d_loss: 1.277432 \tepoch_g_loss: 0.927530\n",
      "Epoch [5/100]:\n",
      "epoch_d_loss: 1.308048 \tepoch_g_loss: 0.998113\n",
      "Epoch [6/100]:\n",
      "epoch_d_loss: 1.207989 \tepoch_g_loss: 1.282239\n",
      "Epoch [7/100]:\n",
      "epoch_d_loss: 1.103022 \tepoch_g_loss: 1.373435\n",
      "Epoch [8/100]:\n",
      "epoch_d_loss: 1.101952 \tepoch_g_loss: 1.352095\n",
      "Epoch [9/100]:\n",
      "epoch_d_loss: 1.002844 \tepoch_g_loss: 1.463169\n",
      "Epoch [10/100]:\n",
      "epoch_d_loss: 1.084136 \tepoch_g_loss: 1.295303\n",
      "Epoch [11/100]:\n",
      "epoch_d_loss: 1.151205 \tepoch_g_loss: 1.132233\n",
      "Epoch [12/100]:\n",
      "epoch_d_loss: 1.190499 \tepoch_g_loss: 1.045228\n",
      "Epoch [13/100]:\n",
      "epoch_d_loss: 1.188032 \tepoch_g_loss: 1.036915\n",
      "Epoch [14/100]:\n",
      "epoch_d_loss: 1.166178 \tepoch_g_loss: 1.097625\n",
      "Epoch [15/100]:\n",
      "epoch_d_loss: 1.234096 \tepoch_g_loss: 0.979365\n",
      "Epoch [16/100]:\n",
      "epoch_d_loss: 1.180492 \tepoch_g_loss: 1.064715\n",
      "Epoch [17/100]:\n",
      "epoch_d_loss: 1.211384 \tepoch_g_loss: 1.018026\n",
      "Epoch [18/100]:\n",
      "epoch_d_loss: 1.210949 \tepoch_g_loss: 1.017099\n",
      "Epoch [19/100]:\n",
      "epoch_d_loss: 1.223260 \tepoch_g_loss: 0.991943\n",
      "Epoch [20/100]:\n",
      "epoch_d_loss: 1.226964 \tepoch_g_loss: 1.006179\n",
      "Epoch [21/100]:\n",
      "epoch_d_loss: 1.211851 \tepoch_g_loss: 0.992509\n",
      "Epoch [22/100]:\n",
      "epoch_d_loss: 1.234237 \tepoch_g_loss: 0.995720\n",
      "Epoch [23/100]:\n",
      "epoch_d_loss: 1.229621 \tepoch_g_loss: 0.971729\n",
      "Epoch [24/100]:\n",
      "epoch_d_loss: 1.220204 \tepoch_g_loss: 1.033777\n",
      "Epoch [25/100]:\n",
      "epoch_d_loss: 1.225596 \tepoch_g_loss: 0.996328\n",
      "Epoch [26/100]:\n",
      "epoch_d_loss: 1.233006 \tepoch_g_loss: 0.991193\n",
      "Epoch [27/100]:\n",
      "epoch_d_loss: 1.223890 \tepoch_g_loss: 0.998695\n",
      "Epoch [28/100]:\n",
      "epoch_d_loss: 1.217190 \tepoch_g_loss: 1.010554\n",
      "Epoch [29/100]:\n",
      "epoch_d_loss: 1.221147 \tepoch_g_loss: 0.994103\n",
      "Epoch [30/100]:\n",
      "epoch_d_loss: 1.230264 \tepoch_g_loss: 0.987502\n",
      "Epoch [31/100]:\n",
      "epoch_d_loss: 1.232292 \tepoch_g_loss: 1.001862\n",
      "Epoch [32/100]:\n",
      "epoch_d_loss: 1.226820 \tepoch_g_loss: 1.006212\n",
      "Epoch [33/100]:\n",
      "epoch_d_loss: 1.221449 \tepoch_g_loss: 0.997292\n",
      "Epoch [34/100]:\n",
      "epoch_d_loss: 1.227423 \tepoch_g_loss: 0.992135\n",
      "Epoch [35/100]:\n",
      "epoch_d_loss: 1.231249 \tepoch_g_loss: 0.979974\n",
      "Epoch [36/100]:\n",
      "epoch_d_loss: 1.242715 \tepoch_g_loss: 0.979323\n",
      "Epoch [37/100]:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Train\u001b[39;00m\n\u001b[1;32m     19\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 20\u001b[0m d_losses, g_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_minst_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 27\u001b[0m, in \u001b[0;36mtrain_minst_gan\u001b[0;34m(d, g, d_optim, g_optim, loss_fn, dl, n_epochs, device, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m d_running_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m g_running_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcurr_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move input batch to available device\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m## ----------------------------------------------------------------\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m## Train discriminator using real and then fake MNIST images,  \u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m## then compute the total-loss and back-propogate the total-loss\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m## ----------------------------------------------------------------\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Reset gradients\u001b[39;49;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/mnist.py:143\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    139\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index])\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = Discriminator(in_features=784, out_features=1)\n",
    "g = Generator(in_features=100, out_features=784)\n",
    "#g = Generator(100, 32, 784)\n",
    "print(d)\n",
    "print()\n",
    "print(g)\n",
    "\n",
    "# Instantiate optimizers\n",
    "d_optim = optim.Adam(d.parameters(), lr=0.002)\n",
    "g_optim = optim.Adam(g.parameters(), lr=0.002)\n",
    "\n",
    "# Instantiate the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Setup device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Train\n",
    "n_epochs = 100\n",
    "d_losses, g_losses = train_minst_gan(d, g, d_optim, g_optim, \n",
    "                                     loss_fn, dl, n_epochs, device,\n",
    "                                     verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
